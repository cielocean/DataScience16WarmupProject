{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration 2\n",
    "This model iteration is used to predict survival rate on Titanic using Python\n",
    "- Overall Highest Score: 0.78947 (from Model Iteration 1)\n",
    "- Highest Score from Model Iteration 2: 0.77990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "Tried out \n",
    "- Various type of regression: Linear, Logistic, Random Forest, Gradient Boosting\n",
    "- Combination of different regressions\n",
    "- Selection of best predictors\n",
    "- Separating female and male data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import re\n",
    "import operator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Importing Data as pandas dataframe\"\"\"\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous iteration, I noticed that assigning values to variables created unwanted relationships. For example, the embarkment ports, S, Q and C does not have correlation but making S = 1, Q = 2, and C = 3 seems like there is a relationship between the ports.\n",
    "For this iteration, I made new columns of binary values so that the model can learn from raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The format functions groups the data in each column into different classification that i've preassigned and numbers them in order.\n",
    "- The divide functions makes the different groups in each columns into distinct columns with binary data (1 if it is classified as the group, 0 otherwise)\n",
    "- The _wo_sex function is for later when I predicted for male and female data separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Functions for formatting data\"\"\"\n",
    "\n",
    "#df_train = pd.read_csv(\"train.csv\")\n",
    "#df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "def format(df):\n",
    "    \"\"\" \n",
    "    formats all the data in the dataframe(df)\n",
    "    \"\"\"\n",
    "    format_age(df)    \n",
    "    format_fare(df)    \n",
    "    format_embarked(df)  \n",
    "    format_sex(df)    \n",
    "    format_familysize(df)\n",
    "    format_namelength(df)\n",
    "    return\n",
    "\n",
    "def format_wo_sex(df):\n",
    "    \"\"\" \n",
    "    formats all the data in the dataframe(df) except sex\n",
    "    \"\"\"\n",
    "    format_age(df)    \n",
    "    format_fare(df)    \n",
    "    format_embarked(df)   \n",
    "    format_familysize(df)\n",
    "    format_namelength(df)\n",
    "    return\n",
    "    \n",
    "def divide_all(df):\n",
    "    \"\"\"\n",
    "    divide formatted data and\n",
    "    returns new feature name as a list\n",
    "    \"\"\"\n",
    "    pclass_column = divide(df, 'Pclass')\n",
    "    age_column = divide(df, 'Age')\n",
    "    fare_column = divide(df, 'Fare')\n",
    "    embarked_column = divide(df, 'Embarked')\n",
    "    sex_column = divide(df, 'Sex')\n",
    "    familysize_column = divide(df, 'FamilySize')\n",
    "    return pclass_column + age_column + fare_column + embarked_column + sex_column + familysize_column\n",
    "\n",
    "def divide_all_wo_sex(df):\n",
    "    \"\"\"\n",
    "    divide formatted data and\n",
    "    returns new feature name as a list\n",
    "    \"\"\"\n",
    "    pclass_column = divide(df, 'Pclass')\n",
    "    age_column = divide(df, 'Age')\n",
    "    fare_column = divide(df, 'Fare')\n",
    "    embarked_column = divide(df, 'Embarked')\n",
    "    familysize_column = divide(df, 'FamilySize')\n",
    "    return pclass_column + age_column + fare_column + embarked_column + familysize_column\n",
    "\n",
    "\n",
    "def format_age(df):\n",
    "    \"\"\"\n",
    "    grouping age into agegroups (years old)\n",
    "    0: unknown age\n",
    "    1: infants (0-1) & toddlers (1-3)\n",
    "    2: preschoolers (3-5)\n",
    "    3: middle childhood (6-11)\n",
    "    4: young teen (12-14)\n",
    "    5: teenagers (15-17)\n",
    "    6: young adults (18-35)\n",
    "    7: middle-aged adults (36-55)\n",
    "    8: young older aduls (55-60)\n",
    "    9: retired (>60)\n",
    "    \n",
    "    \"\"\"\n",
    "    ag = {1:[0,3], 2:[3,5], 3:[5,11], 4:[11,14], 5:[14,17], 6:[17,35], 7:[35,55], 8:[55,60], 9:[60,150]}\n",
    "    df['Age'] = df['Age'].fillna(0)\n",
    "    \n",
    "    for i in range(1,10):\n",
    "        df.loc[(df['Age'] > ag[i][0]) & (df['Age'] <= ag[i][1]), 'Age'] = i\n",
    "    return\n",
    "\n",
    "def format_fare(df):\n",
    "    \"\"\"\n",
    "    grouping fare by amount the passenger paid\n",
    "    0: unknown\n",
    "    1: 0-10\n",
    "    2: 10-20\n",
    "    3: 20-30\n",
    "    4: 30-40\n",
    "    5: 40-550\n",
    "    \"\"\"\n",
    "    fg = {1:[0,10], 2:[10,20], 3:[20,30], 4:[30,40], 5:[40,550]}\n",
    "    df['Fare'] = df['Fare'].fillna(0)\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        df.loc[(df['Fare'] > fg[i][0]) & (df['Fare'] <= fg[i][1]), 'Fare'] = i\n",
    "    return\n",
    "\n",
    "def format_embarked(df):\n",
    "    \"\"\"\n",
    "    change the value from string to float \n",
    "    for dividing up the columns later\n",
    "    0: no data available\n",
    "    1: Southampton\n",
    "    2: Cherbourg\n",
    "    3: Queenstown\n",
    "    \"\"\"\n",
    "    df['Embarked'] = df['Embarked'].fillna(0)\n",
    "    \n",
    "    df.loc[df[\"Embarked\"] == 'S', \"Embarked\"] =1\n",
    "    df.loc[df[\"Embarked\"] == 'C', \"Embarked\"] =2\n",
    "    df.loc[df[\"Embarked\"] == 'Q', \"Embarked\"] =3\n",
    "    return\n",
    "\n",
    "def format_sex(df):\n",
    "    \"\"\"\n",
    "    change the value from string to float \n",
    "    for dividing up the columns later\n",
    "    1: male\n",
    "    2: female\n",
    "    \"\"\"\n",
    "    df.loc[df['Sex'] == 'male', 'Sex'] = 1\n",
    "    df.loc[df['Sex'] == 'female', 'Sex'] = 2\n",
    "    return\n",
    "    \n",
    "def format_familysize(df):\n",
    "    \"\"\"\n",
    "    Generate column named FamilySize \n",
    "    which is the sum of number of family member onboard\n",
    "    \n",
    "    Group by familysize\n",
    "    0: unknown\n",
    "    1: 1\n",
    "    2: 2\n",
    "    3: 3-5\n",
    "    4: >5\n",
    "    5: 0\n",
    "    \"\"\"\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch']\n",
    "    df['FamilySize'] = df['FamilySize'].fillna(0)\n",
    "    \n",
    "    df.loc[(df['FamilySize'] > 2) & (df['FamilySize'] <= 5), 'FamilySize'] = 3\n",
    "    df.loc[df['FamilySize'] > 5, 'FamilySize'] = 4\n",
    "    df.loc[df['FamilySize'] == 0, 'FamilySize'] = 5\n",
    "    return\n",
    "    \n",
    "def format_namelength(df):\n",
    "    \"\"\"\n",
    "    Generate column named NameLength\n",
    "    which shows the length of the name\n",
    "    \"\"\"\n",
    "    df[\"NameLength\"] = df[\"Name\"].apply(lambda x: len(x))\n",
    "    return\n",
    "\n",
    "def divide(df, column):\n",
    "    \"\"\"\n",
    "    Create new columns for column such that the values are 1 or 0\n",
    "    returns name of the newly generated columns(features) in a list\n",
    "    \n",
    "    example:\n",
    "    divide(df, 'Sex')\n",
    "    will result in columns Sex1 and Sex2, such that\n",
    "    Sex1 will have value 1 for all males and 0 for the rest & unknown\n",
    "    Sex2 will have value 1 for all females and 0 for the rest & unknown\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    for i in df[column].unique():\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            column_name = column + str(int(i))\n",
    "            df[column_name] = df[column]\n",
    "            columns.append(column_name)\n",
    "            df.loc[df[column_name] != i, column_name] = 0\n",
    "            df.loc[df[column_name] == i, column_name] = 1\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements that can be made\n",
    "- use get_dummies from pandas\n",
    "- use fit_transform from LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Formatting data\"\"\"\n",
    "format(df_train)\n",
    "format(df_test)\n",
    "train_columns = divide_all(df_train)\n",
    "test_columns = divide_all(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for difference in columns (for test and train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Checking to see if the columns of test data and train data are the same\"\"\"\n",
    "train_columns.sort()\n",
    "test_columns.sort()\n",
    "if train_columns == test_columns:\n",
    "    predictors = train_columns\n",
    "\n",
    "print train_columns == test_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out different types of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804713804714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:26: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Using Linear Regression\"\"\"\n",
    "\n",
    "alg = LinearRegression()\n",
    "kf = KFold(df_train.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm (only take the rows in the train folds)\n",
    "    train_predictors = (df_train[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = df_train[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # make predictions on the test fold\n",
    "    test_predictions = alg.predict(df_train[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# concatenate the predictions in three seperate numpy array on axis 0\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "accuracy = sum(predictions[predictions == df_train[\"Survived\"]]) / len(predictions)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799102132435\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Using LogisticRegression\"\"\"\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds\n",
    "scores = cross_validation.cross_val_score(alg, df_train[predictors], df_train[\"Survived\"], cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817059483726\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Using RandomForest\"\"\"\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "# Compute the accuracy score for all the cross validation folds\n",
    "scores = cross_validation.cross_val_score(alg, df_train[predictors], df_train[\"Survived\"], cv=3)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtained submission score of 0.75120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best features by plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAE2CAYAAACqSMMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHLTNzGrEQdvMrEb+DxlxvzXbf0DaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7197077fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Finding the best features\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "predictors_original = ['Embarked', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'NameLength']\n",
    "predictors = predictors + predictors_original\n",
    "predictors.sort()\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(df_train[predictors], df_train[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores for different features\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the best predictors \n",
    "- Sex (predicted similarly to Sex1 and Sex2 so we can just use sex)\n",
    "- Pclass/ Pclass1 + Pclass2 (Pclass3 does not seem to affect the outcome very much)\n",
    "- Fare/ Fare1 (other fares seem have little effect compared to fare 1)\n",
    "- FamilySize/ FamilySize1 + FamilySize2 + FamilySize5 (either no family, family size 1 and family size 2 affected the prediction the most)\n",
    "- Embarked/ Embarked1 + Embarked2 (knowing Embarked3(Queens) did not affect the outcome very much)\n",
    "- Predicting using certain age groups (1,2,4) is better than predicting with the entire age group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best predictors and applying on RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810325476992\n"
     ]
    }
   ],
   "source": [
    "# Picking out the best features.\n",
    "predictors_filtered = ['Age1', 'Age2', 'Age4', 'Age9',\n",
    "              'Embarked1',\n",
    "              'FamilySize', \n",
    "              'Fare1', 'Fare2', \n",
    "              'NameLength',\n",
    "              'Pclass1', 'Pclass2',\n",
    "              'Sex']\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, df_train[predictors_filtered], df_train[\"Survived\"], cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually decreased the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting using ensembling of RandomForestClassifier and LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Making predictions on test data\"\"\"\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(df_train[predictors], df_train[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(df_test[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810325476992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:36: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Ensembling\"\"\"\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "algorithms = [\n",
    "    #[GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors + predictors_original],\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2), predictors_filtered],\n",
    "    [LogisticRegression(random_state=1), predictors], #Use more linear predictors for the logistic regression\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(df_train.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = df_train[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(df_train[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(df_train[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == df_train[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(df_train[predictors], df_train[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(df_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtained score of 0.76555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": df_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "submission.to_csv('titanic_mi2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying machine learning separately on female and male data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out different model did not improve the score as much as I hoped so I am going to try separating the female and male data again like the Model Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jkim/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\"\"\"dividing male and female data\"\"\"\n",
    "df_male_train = df_train[df_train['Sex'] == 'male']\n",
    "df_female_train = df_train[df_train['Sex'] == 'female']\n",
    "\n",
    "df_male_test = df_test[df_test['Sex'] == 'male']\n",
    "df_female_test = df_test[df_test['Sex'] == 'female']\n",
    "\n",
    "def get_predictors(df_train, df_test):\n",
    "    \"\"\"format data and get predictiors as return value\"\"\"\n",
    "    \n",
    "    \"\"\"Formatting data\"\"\"\n",
    "    format_wo_sex(df_train)\n",
    "    format_wo_sex(df_test)\n",
    "    train_columns = divide_all_wo_sex(df_train)\n",
    "    test_columns = divide_all_wo_sex(df_test)\n",
    "\n",
    "    #\"\"\"Checking to see if the columns of test data and train data are the same\"\"\"\n",
    "    #train_columns.sort()\n",
    "    #test_columns.sort()\n",
    "    #if train_columns == test_columns:\n",
    "    #    predictors = train_columns\n",
    "    #else:\n",
    "    #    predictors = test_columns\n",
    "\n",
    "    #print train_columns == test_columns\n",
    "    \n",
    "    predictors = test_columns\n",
    "    return predictors\n",
    "\n",
    "def set_algorithm(predictors):\n",
    "    algorithms = [\n",
    "        [LogisticRegression, predictors],\n",
    "        [RandomForestClassifier(min_samples_split=4), predictors]\n",
    "    ]\n",
    "    return algorithms\n",
    "\n",
    "\n",
    "def en_prediction(df_train, df_test, algorithms):\n",
    "    full_predictions = []\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm using the full training data.\n",
    "        alg.fit(df_train[predictors], df_train[\"Survived\"])\n",
    "        # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "        predictions = alg.predict_proba(df_test[predictors].astype(float))[:,1]\n",
    "        full_predictions.append(predictions)\n",
    "\n",
    "    # The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "    predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "    predictions[predictions <= .5] = 0\n",
    "    predictions[predictions > .5] = 1\n",
    "    predictions = predictions.astype(int)\n",
    "    return predictions\n",
    "\n",
    "def make_submission(df_test, predictions):\n",
    "    submission = pd.DataFrame({\n",
    "            \"PassengerId\": df_test[\"PassengerId\"],\n",
    "            \"Survived\": predictions\n",
    "        })\n",
    "    return submission\n",
    "\n",
    "\"\"\"Predicting for male\"\"\"\n",
    "#predictors = get_predictors(df_male_train, df_male_test)\n",
    "#algorithms = set_algorithm(predictors)\n",
    "#predictions = en_prediction(df_male_train, df_male_test, algorithms)\n",
    "#submission_male = make_submission(df_male_test, predictions)\n",
    "\n",
    "predictors= get_predictors(df_male_train, df_male_test)\n",
    "alg = RandomForestClassifier(min_samples_split=4, min_samples_leaf=2)\n",
    "alg.fit(df_male_train[predictors], df_male_train[\"Survived\"])\n",
    "predictions = alg.predict(df_male_test[predictors])\n",
    "submission_male = make_submission(df_male_test, predictions)\n",
    "               \n",
    "\"\"\"Predicting for female\"\"\"\n",
    "#predictors = get_predictors(df_female_train, df_female_test)\n",
    "#algorithms = set_algorithm(predictors)\n",
    "#predictions = en_prediction(df_female_train, df_female_test, algorithms)\n",
    "#submission_female = make_submission(df_female_test, predictions)\n",
    "\n",
    "predictors = get_predictors(df_female_train, df_female_test)\n",
    "alg = RandomForestClassifier(min_samples_split=4, min_samples_leaf=2)\n",
    "alg.fit(df_female_train[predictors], df_female_train[\"Survived\"])\n",
    "predictions = alg.predict(df_female_test[predictors])\n",
    "submission_female = make_submission(df_female_test, predictions)\n",
    "\n",
    "\n",
    "frames = [submission_male, submission_female]\n",
    "submission = pd.concat(frames)\n",
    "submission.to_csv('titanic_mi2b.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is slightly higher than prediction made by applying the regression on both female and male.\n",
    "- submission score using Logistic Regression for both female and male: 0.77033\n",
    "- submission score using Random Forest for both female and male: 0.77990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvements that can be made\n",
    "- Selecting out the best predictors for the different dataframe (male/ female) and applying only the selected predictors\n",
    "- Evaluating which regression work best for each dataframe and applying different regressions\n",
    "- Seperating dataframe into young, middle aged, and old and training them separately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataquest\n",
    "- https://www.dataquest.io/mission/74/getting-started-with-kaggle\n",
    "- https://www.dataquest.io/mission/75/improving-your-submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
